{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We offer two datasets (i.e., [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) and [Tiny ImageNet](https://huggingface.co/datasets/Maysee/tiny-imagenet)) to conduct the experiments. For each task, we offer the following three data files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 50000\n",
      "<class 'list'> 2\n",
      "<class 'torch.Tensor'> torch.Size([3, 64, 64])\n",
      "<class 'int'> 119\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'pickle/tinyimagenet/mobilenetv2/shadow.p'\n",
    "\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open(DATA_PATH, \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "print(type(dataset), len(dataset))\n",
    "print(type(dataset[0]), len(dataset[0]))\n",
    "\n",
    "print(type(dataset[0][0]), dataset[0][0].shape)\n",
    "print(type(dataset[0][1]), dataset[0][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24ca6b4b8b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(dataset[0][0].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000\n",
      "15000\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset[:int(0.7*(len(dataset)))]\n",
    "test_dataset = dataset[int(0.7*len(dataset)):]\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# for batch_idx, (img, label) in enumerate(dataloader):\n",
    "#     img = img.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = 'pickle/tinyimagenet/mobilenetv2/eval.p'\n",
    "# # Change the DATA_PATH to your local pickle file path\n",
    "\n",
    "# device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# with open(DATA_PATH, \"rb\") as f:\n",
    "#     dataset = pickle.load(f)\n",
    "\n",
    "# print(type(dataset), len(dataset))\n",
    "# print(type(dataset[0]), len(dataset[0]))\n",
    "\n",
    "# print(type(dataset[0][0]), dataset[0][0].shape)\n",
    "# print(type(dataset[0][1]), dataset[0][1])\n",
    "# print(type(dataset[0][2]), dataset[0][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "# for batch_idx, (img, label, isMemeber) in enumerate(dataloader):\n",
    "#     img = img.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "You need to conduct the experiments on two target model architectures (i.e., [ResNet34](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet34.html) and [MobileNetV2](https://pytorch.org/vision/main/models/generated/torchvision.models.mobilenet_v2.html)). The pre-trained model weights are provided in the `aamlm/models` folder. You can use the example code below to load the model weights for the ResNet34 target model trained on the CIFAR10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_PATH = 'models/mobilenetv2_tinyimagenet.pth'\n",
    "# # MODE_PATH = 'trained_models/tinyimagenet/mobilenetv2_496.pth'\n",
    "# # Change the MODEL_PATH to your local model path\n",
    "\n",
    "# device=torch.devicea(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# target_model = models.mobilenet_v2(num_classes=200).to(device)\n",
    "# # Change num_classes to 200 when you use the Tiny ImageNet dataset\n",
    "\n",
    "# # state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "# # target_model.load_state_dict(state_dict['net'])\n",
    "# # print(state_dict)\n",
    "# print(target_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHADOW MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def eval(test_loader, target_model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    target_model.eval()\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "#             images = transform_val(images)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = target_model(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct // total\n",
    "\n",
    "#     print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\klens\\klens_venv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "d:\\klens\\klens_venv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "# shadow_model = models.resnet34(pretrained=True)\n",
    "\n",
    "shadow_model = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "shadow_model.classifier[1] = torch.nn.Linear(shadow_model.classifier[1].in_features, 200)\n",
    "shadow_model = shadow_model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load trained shadow model if required\n",
    "MODEL_PATH = \"trained_models/tinyimagenet/mobilenetv2_395_scratch.pth\"\n",
    "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "shadow_model.load_state_dict(state_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Eval Shadow Model if required\n",
    "eval(test_dataloader, shadow_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[201,   100] loss: 0.046\n",
      "[201,   200] loss: 0.038\n",
      "[201,   300] loss: 0.043\n",
      "[201,   400] loss: 0.046\n",
      "[201,   500] loss: 0.045\n",
      "============Evaluating Epoch 200========\n",
      "Train Accuracy : 97\n",
      "Test Accuracy : 40\n",
      "[202,   100] loss: 0.036\n",
      "[202,   200] loss: 0.028\n",
      "[202,   300] loss: 0.041\n",
      "[202,   400] loss: 0.040\n",
      "[202,   500] loss: 0.035\n",
      "[203,   100] loss: 0.045\n",
      "[203,   200] loss: 0.040\n",
      "[203,   300] loss: 0.040\n",
      "[203,   400] loss: 0.041\n",
      "[203,   500] loss: 0.042\n",
      "[204,   100] loss: 0.050\n",
      "[204,   200] loss: 0.039\n",
      "[204,   300] loss: 0.037\n",
      "[204,   400] loss: 0.046\n",
      "[204,   500] loss: 0.039\n",
      "[205,   100] loss: 0.033\n",
      "[205,   200] loss: 0.039\n",
      "[205,   300] loss: 0.031\n",
      "[205,   400] loss: 0.048\n",
      "[205,   500] loss: 0.035\n",
      "[206,   100] loss: 0.033\n",
      "[206,   200] loss: 0.041\n",
      "[206,   300] loss: 0.036\n",
      "[206,   400] loss: 0.032\n",
      "[206,   500] loss: 0.032\n",
      "============Evaluating Epoch 205========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 41\n",
      "[207,   100] loss: 0.029\n",
      "[207,   200] loss: 0.028\n",
      "[207,   300] loss: 0.038\n",
      "[207,   400] loss: 0.039\n",
      "[207,   500] loss: 0.039\n",
      "[208,   100] loss: 0.046\n",
      "[208,   200] loss: 0.049\n",
      "[208,   300] loss: 0.045\n",
      "[208,   400] loss: 0.045\n",
      "[208,   500] loss: 0.046\n",
      "[209,   100] loss: 0.048\n",
      "[209,   200] loss: 0.041\n",
      "[209,   300] loss: 0.039\n",
      "[209,   400] loss: 0.039\n",
      "[209,   500] loss: 0.032\n",
      "[210,   100] loss: 0.040\n",
      "[210,   200] loss: 0.039\n",
      "[210,   300] loss: 0.034\n",
      "[210,   400] loss: 0.036\n",
      "[210,   500] loss: 0.042\n",
      "[211,   100] loss: 0.033\n",
      "[211,   200] loss: 0.031\n",
      "[211,   300] loss: 0.040\n",
      "[211,   400] loss: 0.034\n",
      "[211,   500] loss: 0.036\n",
      "============Evaluating Epoch 210========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 40\n",
      "[212,   100] loss: 0.028\n",
      "[212,   200] loss: 0.034\n",
      "[212,   300] loss: 0.041\n",
      "[212,   400] loss: 0.044\n",
      "[212,   500] loss: 0.045\n",
      "[213,   100] loss: 0.044\n",
      "[213,   200] loss: 0.049\n",
      "[213,   300] loss: 0.042\n",
      "[213,   400] loss: 0.042\n",
      "[213,   500] loss: 0.040\n",
      "[214,   100] loss: 0.041\n",
      "[214,   200] loss: 0.035\n",
      "[214,   300] loss: 0.039\n",
      "[214,   400] loss: 0.033\n",
      "[214,   500] loss: 0.033\n",
      "[215,   100] loss: 0.030\n",
      "[215,   200] loss: 0.034\n",
      "[215,   300] loss: 0.043\n",
      "[215,   400] loss: 0.030\n",
      "[215,   500] loss: 0.030\n",
      "[216,   100] loss: 0.032\n",
      "[216,   200] loss: 0.043\n",
      "[216,   300] loss: 0.042\n",
      "[216,   400] loss: 0.032\n",
      "[216,   500] loss: 0.034\n",
      "============Evaluating Epoch 215========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 40\n",
      "[217,   100] loss: 0.032\n",
      "[217,   200] loss: 0.040\n",
      "[217,   300] loss: 0.033\n",
      "[217,   400] loss: 0.029\n",
      "[217,   500] loss: 0.036\n",
      "[218,   100] loss: 0.033\n",
      "[218,   200] loss: 0.035\n",
      "[218,   300] loss: 0.037\n",
      "[218,   400] loss: 0.032\n",
      "[218,   500] loss: 0.034\n",
      "[219,   100] loss: 0.035\n",
      "[219,   200] loss: 0.039\n",
      "[219,   300] loss: 0.041\n",
      "[219,   400] loss: 0.043\n",
      "[219,   500] loss: 0.036\n",
      "[220,   100] loss: 0.045\n",
      "[220,   200] loss: 0.032\n",
      "[220,   300] loss: 0.036\n",
      "[220,   400] loss: 0.040\n",
      "[220,   500] loss: 0.046\n",
      "[221,   100] loss: 0.048\n",
      "[221,   200] loss: 0.034\n",
      "[221,   300] loss: 0.037\n",
      "[221,   400] loss: 0.028\n",
      "[221,   500] loss: 0.031\n",
      "============Evaluating Epoch 220========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 40\n",
      "[222,   100] loss: 0.026\n",
      "[222,   200] loss: 0.028\n",
      "[222,   300] loss: 0.032\n",
      "[222,   400] loss: 0.033\n",
      "[222,   500] loss: 0.030\n",
      "[223,   100] loss: 0.033\n",
      "[223,   200] loss: 0.036\n",
      "[223,   300] loss: 0.043\n",
      "[223,   400] loss: 0.040\n",
      "[223,   500] loss: 0.041\n",
      "[224,   100] loss: 0.041\n",
      "[224,   200] loss: 0.042\n",
      "[224,   300] loss: 0.028\n",
      "[224,   400] loss: 0.028\n",
      "[224,   500] loss: 0.032\n",
      "[225,   100] loss: 0.037\n",
      "[225,   200] loss: 0.037\n",
      "[225,   300] loss: 0.030\n",
      "[225,   400] loss: 0.028\n",
      "[225,   500] loss: 0.030\n",
      "[226,   100] loss: 0.030\n",
      "[226,   200] loss: 0.029\n",
      "[226,   300] loss: 0.036\n",
      "[226,   400] loss: 0.036\n",
      "[226,   500] loss: 0.037\n",
      "============Evaluating Epoch 225========\n",
      "Train Accuracy : 97\n",
      "Test Accuracy : 40\n",
      "[227,   100] loss: 0.029\n",
      "[227,   200] loss: 0.041\n",
      "[227,   300] loss: 0.039\n",
      "[227,   400] loss: 0.042\n",
      "[227,   500] loss: 0.032\n",
      "[228,   100] loss: 0.032\n",
      "[228,   200] loss: 0.034\n",
      "[228,   300] loss: 0.041\n",
      "[228,   400] loss: 0.033\n",
      "[228,   500] loss: 0.025\n",
      "[229,   100] loss: 0.045\n",
      "[229,   200] loss: 0.043\n",
      "[229,   300] loss: 0.048\n",
      "[229,   400] loss: 0.050\n",
      "[229,   500] loss: 0.049\n",
      "[230,   100] loss: 0.035\n",
      "[230,   200] loss: 0.039\n",
      "[230,   300] loss: 0.028\n",
      "[230,   400] loss: 0.032\n",
      "[230,   500] loss: 0.029\n",
      "[231,   100] loss: 0.028\n",
      "[231,   200] loss: 0.020\n",
      "[231,   300] loss: 0.025\n",
      "[231,   400] loss: 0.032\n",
      "[231,   500] loss: 0.036\n",
      "============Evaluating Epoch 230========\n",
      "Train Accuracy : 97\n",
      "Test Accuracy : 39\n",
      "[232,   100] loss: 0.040\n",
      "[232,   200] loss: 0.041\n",
      "[232,   300] loss: 0.041\n",
      "[232,   400] loss: 0.037\n",
      "[232,   500] loss: 0.040\n",
      "[233,   100] loss: 0.046\n",
      "[233,   200] loss: 0.043\n",
      "[233,   300] loss: 0.037\n",
      "[233,   400] loss: 0.045\n",
      "[233,   500] loss: 0.046\n",
      "[234,   100] loss: 0.045\n",
      "[234,   200] loss: 0.036\n",
      "[234,   300] loss: 0.025\n",
      "[234,   400] loss: 0.025\n",
      "[234,   500] loss: 0.033\n",
      "[235,   100] loss: 0.038\n",
      "[235,   200] loss: 0.036\n",
      "[235,   300] loss: 0.033\n",
      "[235,   400] loss: 0.032\n",
      "[235,   500] loss: 0.035\n",
      "[236,   100] loss: 0.025\n",
      "[236,   200] loss: 0.023\n",
      "[236,   300] loss: 0.037\n",
      "[236,   400] loss: 0.030\n",
      "[236,   500] loss: 0.028\n",
      "============Evaluating Epoch 235========\n",
      "Train Accuracy : 97\n",
      "Test Accuracy : 39\n",
      "[237,   100] loss: 0.046\n",
      "[237,   200] loss: 0.036\n",
      "[237,   300] loss: 0.032\n",
      "[237,   400] loss: 0.032\n",
      "[237,   500] loss: 0.033\n",
      "[238,   100] loss: 0.035\n",
      "[238,   200] loss: 0.038\n",
      "[238,   300] loss: 0.030\n",
      "[238,   400] loss: 0.028\n",
      "[238,   500] loss: 0.041\n",
      "[239,   100] loss: 0.031\n",
      "[239,   200] loss: 0.026\n",
      "[239,   300] loss: 0.028\n",
      "[239,   400] loss: 0.031\n",
      "[239,   500] loss: 0.029\n",
      "[240,   100] loss: 0.032\n",
      "[240,   200] loss: 0.037\n",
      "[240,   300] loss: 0.032\n",
      "[240,   400] loss: 0.026\n",
      "[240,   500] loss: 0.035\n",
      "[241,   100] loss: 0.027\n",
      "[241,   200] loss: 0.040\n",
      "[241,   300] loss: 0.030\n",
      "[241,   400] loss: 0.035\n",
      "[241,   500] loss: 0.048\n",
      "============Evaluating Epoch 240========\n",
      "Train Accuracy : 97\n",
      "Test Accuracy : 39\n",
      "[242,   100] loss: 0.037\n",
      "[242,   200] loss: 0.039\n",
      "[242,   300] loss: 0.029\n",
      "[242,   400] loss: 0.042\n",
      "[242,   500] loss: 0.047\n",
      "[243,   100] loss: 0.036\n",
      "[243,   200] loss: 0.037\n",
      "[243,   300] loss: 0.038\n",
      "[243,   400] loss: 0.032\n",
      "[243,   500] loss: 0.034\n",
      "[244,   100] loss: 0.025\n",
      "[244,   200] loss: 0.023\n",
      "[244,   300] loss: 0.024\n",
      "[244,   400] loss: 0.022\n",
      "[244,   500] loss: 0.022\n",
      "[245,   100] loss: 0.022\n",
      "[245,   200] loss: 0.018\n",
      "[245,   300] loss: 0.021\n",
      "[245,   400] loss: 0.023\n",
      "[245,   500] loss: 0.020\n",
      "[246,   100] loss: 0.024\n",
      "[246,   200] loss: 0.041\n",
      "[246,   300] loss: 0.040\n",
      "[246,   400] loss: 0.034\n",
      "[246,   500] loss: 0.040\n",
      "============Evaluating Epoch 245========\n",
      "Train Accuracy : 97\n",
      "Test Accuracy : 39\n",
      "[247,   100] loss: 0.047\n",
      "[247,   200] loss: 0.052\n",
      "[247,   300] loss: 0.057\n",
      "[247,   400] loss: 0.037\n",
      "[247,   500] loss: 0.042\n",
      "[248,   100] loss: 0.047\n",
      "[248,   200] loss: 0.038\n",
      "[248,   300] loss: 0.039\n",
      "[248,   400] loss: 0.038\n",
      "[248,   500] loss: 0.034\n",
      "[249,   100] loss: 0.033\n",
      "[249,   200] loss: 0.028\n",
      "[249,   300] loss: 0.023\n",
      "[249,   400] loss: 0.016\n",
      "[249,   500] loss: 0.021\n",
      "[250,   100] loss: 0.020\n",
      "[250,   200] loss: 0.020\n",
      "[250,   300] loss: 0.018\n",
      "[250,   400] loss: 0.023\n",
      "[250,   500] loss: 0.020\n",
      "[251,   100] loss: 0.030\n",
      "[251,   200] loss: 0.037\n",
      "[251,   300] loss: 0.034\n",
      "[251,   400] loss: 0.041\n",
      "[251,   500] loss: 0.042\n",
      "============Evaluating Epoch 250========\n",
      "Train Accuracy : 97\n",
      "Test Accuracy : 39\n",
      "[252,   100] loss: 0.044\n",
      "[252,   200] loss: 0.042\n",
      "[252,   300] loss: 0.050\n",
      "[252,   400] loss: 0.044\n",
      "[252,   500] loss: 0.037\n",
      "[253,   100] loss: 0.040\n",
      "[253,   200] loss: 0.034\n",
      "[253,   300] loss: 0.027\n",
      "[253,   400] loss: 0.031\n",
      "[253,   500] loss: 0.031\n",
      "[254,   100] loss: 0.028\n",
      "[254,   200] loss: 0.036\n",
      "[254,   300] loss: 0.026\n",
      "[254,   400] loss: 0.030\n",
      "[254,   500] loss: 0.027\n",
      "[255,   100] loss: 0.037\n",
      "[255,   200] loss: 0.035\n",
      "[255,   300] loss: 0.030\n",
      "[255,   400] loss: 0.032\n",
      "[255,   500] loss: 0.030\n",
      "[256,   100] loss: 0.028\n",
      "[256,   200] loss: 0.024\n",
      "[256,   300] loss: 0.028\n",
      "[256,   400] loss: 0.027\n",
      "[256,   500] loss: 0.034\n",
      "============Evaluating Epoch 255========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 40\n",
      "[257,   100] loss: 0.023\n",
      "[257,   200] loss: 0.026\n",
      "[257,   300] loss: 0.023\n",
      "[257,   400] loss: 0.032\n",
      "[257,   500] loss: 0.036\n",
      "[258,   100] loss: 0.042\n",
      "[258,   200] loss: 0.035\n",
      "[258,   300] loss: 0.028\n",
      "[258,   400] loss: 0.033\n",
      "[258,   500] loss: 0.035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[259,   100] loss: 0.037\n",
      "[259,   200] loss: 0.026\n",
      "[259,   300] loss: 0.033\n",
      "[259,   400] loss: 0.032\n",
      "[259,   500] loss: 0.036\n",
      "[260,   100] loss: 0.030\n",
      "[260,   200] loss: 0.022\n",
      "[260,   300] loss: 0.025\n",
      "[260,   400] loss: 0.024\n",
      "[260,   500] loss: 0.024\n",
      "[261,   100] loss: 0.019\n",
      "[261,   200] loss: 0.024\n",
      "[261,   300] loss: 0.029\n",
      "[261,   400] loss: 0.028\n",
      "[261,   500] loss: 0.024\n",
      "============Evaluating Epoch 260========\n",
      "Train Accuracy : 97\n",
      "Test Accuracy : 39\n",
      "[262,   100] loss: 0.026\n",
      "[262,   200] loss: 0.028\n",
      "[262,   300] loss: 0.026\n",
      "[262,   400] loss: 0.032\n",
      "[262,   500] loss: 0.025\n",
      "[263,   100] loss: 0.036\n",
      "[263,   200] loss: 0.041\n",
      "[263,   300] loss: 0.046\n",
      "[263,   400] loss: 0.050\n",
      "[263,   500] loss: 0.043\n",
      "[264,   100] loss: 0.030\n",
      "[264,   200] loss: 0.034\n",
      "[264,   300] loss: 0.039\n",
      "[264,   400] loss: 0.028\n",
      "[264,   500] loss: 0.024\n",
      "[265,   100] loss: 0.023\n",
      "[265,   200] loss: 0.022\n",
      "[265,   300] loss: 0.025\n",
      "[265,   400] loss: 0.019\n",
      "[265,   500] loss: 0.032\n",
      "[266,   100] loss: 0.033\n",
      "[266,   200] loss: 0.028\n",
      "[266,   300] loss: 0.023\n",
      "[266,   400] loss: 0.024\n",
      "[266,   500] loss: 0.027\n",
      "============Evaluating Epoch 265========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 40\n",
      "[267,   100] loss: 0.035\n",
      "[267,   200] loss: 0.042\n",
      "[267,   300] loss: 0.037\n",
      "[267,   400] loss: 0.038\n",
      "[267,   500] loss: 0.027\n",
      "[268,   100] loss: 0.031\n",
      "[268,   200] loss: 0.033\n",
      "[268,   300] loss: 0.035\n",
      "[268,   400] loss: 0.031\n",
      "[268,   500] loss: 0.027\n",
      "[269,   100] loss: 0.022\n",
      "[269,   200] loss: 0.029\n",
      "[269,   300] loss: 0.020\n",
      "[269,   400] loss: 0.018\n",
      "[269,   500] loss: 0.024\n",
      "[270,   100] loss: 0.023\n",
      "[270,   200] loss: 0.024\n",
      "[270,   300] loss: 0.024\n",
      "[270,   400] loss: 0.033\n",
      "[270,   500] loss: 0.024\n",
      "[271,   100] loss: 0.033\n",
      "[271,   200] loss: 0.036\n",
      "[271,   300] loss: 0.032\n",
      "[271,   400] loss: 0.041\n",
      "[271,   500] loss: 0.042\n",
      "============Evaluating Epoch 270========\n",
      "Train Accuracy : 97\n",
      "Test Accuracy : 39\n",
      "[272,   100] loss: 0.049\n",
      "[272,   200] loss: 0.045\n",
      "[272,   300] loss: 0.044\n",
      "[272,   400] loss: 0.041\n",
      "[272,   500] loss: 0.035\n",
      "[273,   100] loss: 0.034\n",
      "[273,   200] loss: 0.031\n",
      "[273,   300] loss: 0.027\n",
      "[273,   400] loss: 0.028\n",
      "[273,   500] loss: 0.019\n",
      "[274,   100] loss: 0.019\n",
      "[274,   200] loss: 0.019\n",
      "[274,   300] loss: 0.018\n",
      "[274,   400] loss: 0.020\n",
      "[274,   500] loss: 0.014\n",
      "[275,   100] loss: 0.015\n",
      "[275,   200] loss: 0.022\n",
      "[275,   300] loss: 0.026\n",
      "[275,   400] loss: 0.020\n",
      "[275,   500] loss: 0.025\n",
      "[276,   100] loss: 0.035\n",
      "[276,   200] loss: 0.029\n",
      "[276,   300] loss: 0.029\n",
      "[276,   400] loss: 0.044\n",
      "[276,   500] loss: 0.038\n",
      "============Evaluating Epoch 275========\n",
      "Train Accuracy : 97\n",
      "Test Accuracy : 39\n",
      "[277,   100] loss: 0.044\n",
      "[277,   200] loss: 0.036\n",
      "[277,   300] loss: 0.046\n",
      "[277,   400] loss: 0.038\n",
      "[277,   500] loss: 0.043\n",
      "[278,   100] loss: 0.037\n",
      "[278,   200] loss: 0.034\n",
      "[278,   300] loss: 0.032\n",
      "[278,   400] loss: 0.038\n",
      "[278,   500] loss: 0.033\n",
      "[279,   100] loss: 0.026\n",
      "[279,   200] loss: 0.020\n",
      "[279,   300] loss: 0.020\n",
      "[279,   400] loss: 0.023\n",
      "[279,   500] loss: 0.021\n",
      "[280,   100] loss: 0.021\n",
      "[280,   200] loss: 0.022\n",
      "[280,   300] loss: 0.024\n",
      "[280,   400] loss: 0.022\n",
      "[280,   500] loss: 0.018\n",
      "[281,   100] loss: 0.024\n",
      "[281,   200] loss: 0.023\n",
      "[281,   300] loss: 0.027\n",
      "[281,   400] loss: 0.023\n",
      "[281,   500] loss: 0.031\n",
      "============Evaluating Epoch 280========\n",
      "Train Accuracy : 97\n",
      "Test Accuracy : 39\n",
      "[282,   100] loss: 0.029\n",
      "[282,   200] loss: 0.034\n",
      "[282,   300] loss: 0.034\n",
      "[282,   400] loss: 0.041\n",
      "[282,   500] loss: 0.030\n",
      "[283,   100] loss: 0.033\n",
      "[283,   200] loss: 0.029\n",
      "[283,   300] loss: 0.028\n",
      "[283,   400] loss: 0.024\n",
      "[283,   500] loss: 0.026\n",
      "[284,   100] loss: 0.025\n",
      "[284,   200] loss: 0.025\n",
      "[284,   300] loss: 0.023\n",
      "[284,   400] loss: 0.037\n",
      "[284,   500] loss: 0.028\n",
      "[285,   100] loss: 0.026\n",
      "[285,   200] loss: 0.029\n",
      "[285,   300] loss: 0.027\n",
      "[285,   400] loss: 0.027\n",
      "[285,   500] loss: 0.037\n",
      "[286,   100] loss: 0.031\n",
      "[286,   200] loss: 0.031\n",
      "[286,   300] loss: 0.030\n",
      "[286,   400] loss: 0.028\n",
      "[286,   500] loss: 0.030\n",
      "============Evaluating Epoch 285========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 40\n",
      "[287,   100] loss: 0.022\n",
      "[287,   200] loss: 0.021\n",
      "[287,   300] loss: 0.022\n",
      "[287,   400] loss: 0.026\n",
      "[287,   500] loss: 0.025\n",
      "[288,   100] loss: 0.021\n",
      "[288,   200] loss: 0.018\n",
      "[288,   300] loss: 0.027\n",
      "[288,   400] loss: 0.020\n",
      "[288,   500] loss: 0.020\n",
      "[289,   100] loss: 0.019\n",
      "[289,   200] loss: 0.021\n",
      "[289,   300] loss: 0.021\n",
      "[289,   400] loss: 0.019\n",
      "[289,   500] loss: 0.026\n",
      "[290,   100] loss: 0.025\n",
      "[290,   200] loss: 0.029\n",
      "[290,   300] loss: 0.030\n",
      "[290,   400] loss: 0.035\n",
      "[290,   500] loss: 0.040\n",
      "[291,   100] loss: 0.039\n",
      "[291,   200] loss: 0.035\n",
      "[291,   300] loss: 0.041\n",
      "[291,   400] loss: 0.030\n",
      "[291,   500] loss: 0.031\n",
      "============Evaluating Epoch 290========\n",
      "Train Accuracy : 97\n",
      "Test Accuracy : 39\n",
      "[292,   100] loss: 0.033\n",
      "[292,   200] loss: 0.028\n",
      "[292,   300] loss: 0.030\n",
      "[292,   400] loss: 0.018\n",
      "[292,   500] loss: 0.024\n",
      "[293,   100] loss: 0.023\n",
      "[293,   200] loss: 0.025\n",
      "[293,   300] loss: 0.017\n",
      "[293,   400] loss: 0.028\n",
      "[293,   500] loss: 0.023\n",
      "[294,   100] loss: 0.021\n",
      "[294,   200] loss: 0.024\n",
      "[294,   300] loss: 0.023\n",
      "[294,   400] loss: 0.030\n",
      "[294,   500] loss: 0.024\n",
      "[295,   100] loss: 0.020\n",
      "[295,   200] loss: 0.030\n",
      "[295,   300] loss: 0.028\n",
      "[295,   400] loss: 0.038\n",
      "[295,   500] loss: 0.040\n",
      "[296,   100] loss: 0.026\n",
      "[296,   200] loss: 0.027\n",
      "[296,   300] loss: 0.034\n",
      "[296,   400] loss: 0.037\n",
      "[296,   500] loss: 0.042\n",
      "============Evaluating Epoch 295========\n",
      "Train Accuracy : 97\n",
      "Test Accuracy : 39\n",
      "[297,   100] loss: 0.028\n",
      "[297,   200] loss: 0.035\n",
      "[297,   300] loss: 0.036\n",
      "[297,   400] loss: 0.024\n",
      "[297,   500] loss: 0.021\n",
      "[298,   100] loss: 0.022\n",
      "[298,   200] loss: 0.024\n",
      "[298,   300] loss: 0.021\n",
      "[298,   400] loss: 0.027\n",
      "[298,   500] loss: 0.027\n",
      "[299,   100] loss: 0.021\n",
      "[299,   200] loss: 0.019\n",
      "[299,   300] loss: 0.019\n",
      "[299,   400] loss: 0.021\n",
      "[299,   500] loss: 0.024\n",
      "[300,   100] loss: 0.014\n",
      "[300,   200] loss: 0.018\n",
      "[300,   300] loss: 0.018\n",
      "[300,   400] loss: 0.024\n",
      "[300,   500] loss: 0.023\n",
      "[301,   100] loss: 0.020\n",
      "[301,   200] loss: 0.033\n",
      "[301,   300] loss: 0.026\n",
      "[301,   400] loss: 0.022\n",
      "[301,   500] loss: 0.033\n",
      "============Evaluating Epoch 300========\n",
      "Train Accuracy : 97\n",
      "Test Accuracy : 39\n",
      "[302,   100] loss: 0.031\n",
      "[302,   200] loss: 0.040\n",
      "[302,   300] loss: 0.037\n",
      "[302,   400] loss: 0.031\n",
      "[302,   500] loss: 0.049\n",
      "[303,   100] loss: 0.043\n",
      "[303,   200] loss: 0.035\n",
      "[303,   300] loss: 0.040\n",
      "[303,   400] loss: 0.026\n",
      "[303,   500] loss: 0.015\n",
      "[304,   100] loss: 0.021\n",
      "[304,   200] loss: 0.017\n",
      "[304,   300] loss: 0.019\n",
      "[304,   400] loss: 0.021\n",
      "[304,   500] loss: 0.020\n",
      "[305,   100] loss: 0.020\n",
      "[305,   200] loss: 0.019\n",
      "[305,   300] loss: 0.024\n",
      "[305,   400] loss: 0.022\n",
      "[305,   500] loss: 0.032\n",
      "[306,   100] loss: 0.030\n",
      "[306,   200] loss: 0.035\n",
      "[306,   300] loss: 0.024\n",
      "[306,   400] loss: 0.027\n",
      "[306,   500] loss: 0.032\n",
      "============Evaluating Epoch 305========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 40\n",
      "[307,   100] loss: 0.027\n",
      "[307,   200] loss: 0.031\n",
      "[307,   300] loss: 0.026\n",
      "[307,   400] loss: 0.026\n",
      "[307,   500] loss: 0.025\n",
      "[308,   100] loss: 0.026\n",
      "[308,   200] loss: 0.025\n",
      "[308,   300] loss: 0.023\n",
      "[308,   400] loss: 0.029\n",
      "[308,   500] loss: 0.027\n",
      "[309,   100] loss: 0.022\n",
      "[309,   200] loss: 0.019\n",
      "[309,   300] loss: 0.020\n",
      "[309,   400] loss: 0.022\n",
      "[309,   500] loss: 0.030\n",
      "[310,   100] loss: 0.015\n",
      "[310,   200] loss: 0.017\n",
      "[310,   300] loss: 0.017\n",
      "[310,   400] loss: 0.020\n",
      "[310,   500] loss: 0.020\n",
      "[311,   100] loss: 0.027\n",
      "[311,   200] loss: 0.027\n",
      "[311,   300] loss: 0.026\n",
      "[311,   400] loss: 0.030\n",
      "[311,   500] loss: 0.029\n",
      "============Evaluating Epoch 310========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 40\n",
      "[312,   100] loss: 0.023\n",
      "[312,   200] loss: 0.022\n",
      "[312,   300] loss: 0.031\n",
      "[312,   400] loss: 0.027\n",
      "[312,   500] loss: 0.032\n",
      "[313,   100] loss: 0.025\n",
      "[313,   200] loss: 0.030\n",
      "[313,   300] loss: 0.031\n",
      "[313,   400] loss: 0.036\n",
      "[313,   500] loss: 0.038\n",
      "[314,   100] loss: 0.023\n",
      "[314,   200] loss: 0.025\n",
      "[314,   300] loss: 0.023\n",
      "[314,   400] loss: 0.019\n",
      "[314,   500] loss: 0.018\n",
      "[315,   100] loss: 0.021\n",
      "[315,   200] loss: 0.017\n",
      "[315,   300] loss: 0.020\n",
      "[315,   400] loss: 0.020\n",
      "[315,   500] loss: 0.015\n",
      "[316,   100] loss: 0.014\n",
      "[316,   200] loss: 0.014\n",
      "[316,   300] loss: 0.021\n",
      "[316,   400] loss: 0.024\n",
      "[316,   500] loss: 0.021\n",
      "============Evaluating Epoch 315========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[317,   100] loss: 0.023\n",
      "[317,   200] loss: 0.023\n",
      "[317,   300] loss: 0.028\n",
      "[317,   400] loss: 0.033\n",
      "[317,   500] loss: 0.026\n",
      "[318,   100] loss: 0.031\n",
      "[318,   200] loss: 0.038\n",
      "[318,   300] loss: 0.041\n",
      "[318,   400] loss: 0.036\n",
      "[318,   500] loss: 0.026\n",
      "[319,   100] loss: 0.027\n",
      "[319,   200] loss: 0.028\n",
      "[319,   300] loss: 0.030\n",
      "[319,   400] loss: 0.023\n",
      "[319,   500] loss: 0.028\n",
      "[320,   100] loss: 0.022\n",
      "[320,   200] loss: 0.025\n",
      "[320,   300] loss: 0.020\n",
      "[320,   400] loss: 0.020\n",
      "[320,   500] loss: 0.026\n",
      "[321,   100] loss: 0.015\n",
      "[321,   200] loss: 0.031\n",
      "[321,   300] loss: 0.027\n",
      "[321,   400] loss: 0.026\n",
      "[321,   500] loss: 0.030\n",
      "============Evaluating Epoch 320========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 40\n",
      "[322,   100] loss: 0.033\n",
      "[322,   200] loss: 0.027\n",
      "[322,   300] loss: 0.028\n",
      "[322,   400] loss: 0.024\n",
      "[322,   500] loss: 0.026\n",
      "[323,   100] loss: 0.025\n",
      "[323,   200] loss: 0.024\n",
      "[323,   300] loss: 0.023\n",
      "[323,   400] loss: 0.026\n",
      "[323,   500] loss: 0.031\n",
      "[324,   100] loss: 0.022\n",
      "[324,   200] loss: 0.019\n",
      "[324,   300] loss: 0.016\n",
      "[324,   400] loss: 0.018\n",
      "[324,   500] loss: 0.016\n",
      "[325,   100] loss: 0.018\n",
      "[325,   200] loss: 0.017\n",
      "[325,   300] loss: 0.018\n",
      "[325,   400] loss: 0.022\n",
      "[325,   500] loss: 0.023\n",
      "[326,   100] loss: 0.028\n",
      "[326,   200] loss: 0.031\n",
      "[326,   300] loss: 0.020\n",
      "[326,   400] loss: 0.025\n",
      "[326,   500] loss: 0.035\n",
      "============Evaluating Epoch 325========\n",
      "Train Accuracy : 97\n",
      "Test Accuracy : 39\n",
      "[327,   100] loss: 0.042\n",
      "[327,   200] loss: 0.039\n",
      "[327,   300] loss: 0.028\n",
      "[327,   400] loss: 0.024\n",
      "[327,   500] loss: 0.037\n",
      "[328,   100] loss: 0.035\n",
      "[328,   200] loss: 0.026\n",
      "[328,   300] loss: 0.019\n",
      "[328,   400] loss: 0.020\n",
      "[328,   500] loss: 0.016\n",
      "[329,   100] loss: 0.014\n",
      "[329,   200] loss: 0.013\n",
      "[329,   300] loss: 0.015\n",
      "[329,   400] loss: 0.016\n",
      "[329,   500] loss: 0.019\n",
      "[330,   100] loss: 0.017\n",
      "[330,   200] loss: 0.023\n",
      "[330,   300] loss: 0.028\n",
      "[330,   400] loss: 0.026\n",
      "[330,   500] loss: 0.026\n",
      "[331,   100] loss: 0.021\n",
      "[331,   200] loss: 0.032\n",
      "[331,   300] loss: 0.025\n",
      "[331,   400] loss: 0.034\n",
      "[331,   500] loss: 0.026\n",
      "============Evaluating Epoch 330========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 39\n",
      "[332,   100] loss: 0.020\n",
      "[332,   200] loss: 0.029\n",
      "[332,   300] loss: 0.032\n",
      "[332,   400] loss: 0.035\n",
      "[332,   500] loss: 0.026\n",
      "[333,   100] loss: 0.028\n",
      "[333,   200] loss: 0.033\n",
      "[333,   300] loss: 0.029\n",
      "[333,   400] loss: 0.025\n",
      "[333,   500] loss: 0.019\n",
      "[334,   100] loss: 0.019\n",
      "[334,   200] loss: 0.011\n",
      "[334,   300] loss: 0.020\n",
      "[334,   400] loss: 0.015\n",
      "[334,   500] loss: 0.014\n",
      "[335,   100] loss: 0.022\n",
      "[335,   200] loss: 0.022\n",
      "[335,   300] loss: 0.021\n",
      "[335,   400] loss: 0.026\n",
      "[335,   500] loss: 0.028\n",
      "[336,   100] loss: 0.030\n",
      "[336,   200] loss: 0.033\n",
      "[336,   300] loss: 0.037\n",
      "[336,   400] loss: 0.040\n",
      "[336,   500] loss: 0.037\n",
      "============Evaluating Epoch 335========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 39\n",
      "[337,   100] loss: 0.034\n",
      "[337,   200] loss: 0.025\n",
      "[337,   300] loss: 0.027\n",
      "[337,   400] loss: 0.020\n",
      "[337,   500] loss: 0.017\n",
      "[338,   100] loss: 0.016\n",
      "[338,   200] loss: 0.014\n",
      "[338,   300] loss: 0.018\n",
      "[338,   400] loss: 0.017\n",
      "[338,   500] loss: 0.014\n",
      "[339,   100] loss: 0.023\n",
      "[339,   200] loss: 0.028\n",
      "[339,   300] loss: 0.028\n",
      "[339,   400] loss: 0.033\n",
      "[339,   500] loss: 0.028\n",
      "[340,   100] loss: 0.020\n",
      "[340,   200] loss: 0.025\n",
      "[340,   300] loss: 0.024\n",
      "[340,   400] loss: 0.027\n",
      "[340,   500] loss: 0.024\n",
      "[341,   100] loss: 0.018\n",
      "[341,   200] loss: 0.019\n",
      "[341,   300] loss: 0.017\n",
      "[341,   400] loss: 0.019\n",
      "[341,   500] loss: 0.019\n",
      "============Evaluating Epoch 340========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 40\n",
      "[342,   100] loss: 0.021\n",
      "[342,   200] loss: 0.016\n",
      "[342,   300] loss: 0.023\n",
      "[342,   400] loss: 0.023\n",
      "[342,   500] loss: 0.024\n",
      "[343,   100] loss: 0.023\n",
      "[343,   200] loss: 0.025\n",
      "[343,   300] loss: 0.032\n",
      "[343,   400] loss: 0.027\n",
      "[343,   500] loss: 0.025\n",
      "[344,   100] loss: 0.026\n",
      "[344,   200] loss: 0.020\n",
      "[344,   300] loss: 0.026\n",
      "[344,   400] loss: 0.029\n",
      "[344,   500] loss: 0.025\n",
      "[345,   100] loss: 0.023\n",
      "[345,   200] loss: 0.019\n",
      "[345,   300] loss: 0.019\n",
      "[345,   400] loss: 0.018\n",
      "[345,   500] loss: 0.021\n",
      "[346,   100] loss: 0.026\n",
      "[346,   200] loss: 0.030\n",
      "[346,   300] loss: 0.022\n",
      "[346,   400] loss: 0.024\n",
      "[346,   500] loss: 0.023\n",
      "============Evaluating Epoch 345========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 40\n",
      "[347,   100] loss: 0.024\n",
      "[347,   200] loss: 0.023\n",
      "[347,   300] loss: 0.021\n",
      "[347,   400] loss: 0.016\n",
      "[347,   500] loss: 0.022\n",
      "[348,   100] loss: 0.022\n",
      "[348,   200] loss: 0.027\n",
      "[348,   300] loss: 0.026\n",
      "[348,   400] loss: 0.021\n",
      "[348,   500] loss: 0.023\n",
      "[349,   100] loss: 0.021\n",
      "[349,   200] loss: 0.022\n",
      "[349,   300] loss: 0.025\n",
      "[349,   400] loss: 0.025\n",
      "[349,   500] loss: 0.019\n",
      "[350,   100] loss: 0.014\n",
      "[350,   200] loss: 0.014\n",
      "[350,   300] loss: 0.018\n",
      "[350,   400] loss: 0.018\n",
      "[350,   500] loss: 0.018\n",
      "[351,   100] loss: 0.018\n",
      "[351,   200] loss: 0.018\n",
      "[351,   300] loss: 0.014\n",
      "[351,   400] loss: 0.014\n",
      "[351,   500] loss: 0.021\n",
      "============Evaluating Epoch 350========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 40\n",
      "[352,   100] loss: 0.034\n",
      "[352,   200] loss: 0.028\n",
      "[352,   300] loss: 0.040\n",
      "[352,   400] loss: 0.041\n",
      "[352,   500] loss: 0.035\n",
      "[353,   100] loss: 0.034\n",
      "[353,   200] loss: 0.029\n",
      "[353,   300] loss: 0.028\n",
      "[353,   400] loss: 0.028\n",
      "[353,   500] loss: 0.026\n",
      "[354,   100] loss: 0.031\n",
      "[354,   200] loss: 0.023\n",
      "[354,   300] loss: 0.025\n",
      "[354,   400] loss: 0.027\n",
      "[354,   500] loss: 0.029\n",
      "[355,   100] loss: 0.016\n",
      "[355,   200] loss: 0.014\n",
      "[355,   300] loss: 0.017\n",
      "[355,   400] loss: 0.012\n",
      "[355,   500] loss: 0.012\n",
      "[356,   100] loss: 0.012\n",
      "[356,   200] loss: 0.007\n",
      "[356,   300] loss: 0.011\n",
      "[356,   400] loss: 0.019\n",
      "[356,   500] loss: 0.018\n",
      "============Evaluating Epoch 355========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 40\n",
      "[357,   100] loss: 0.021\n",
      "[357,   200] loss: 0.021\n",
      "[357,   300] loss: 0.029\n",
      "[357,   400] loss: 0.039\n",
      "[357,   500] loss: 0.043\n",
      "[358,   100] loss: 0.035\n",
      "[358,   200] loss: 0.034\n",
      "[358,   300] loss: 0.022\n",
      "[358,   400] loss: 0.029\n",
      "[358,   500] loss: 0.023\n",
      "[359,   100] loss: 0.022\n",
      "[359,   200] loss: 0.019\n",
      "[359,   300] loss: 0.019\n",
      "[359,   400] loss: 0.021\n",
      "[359,   500] loss: 0.019\n",
      "[360,   100] loss: 0.013\n",
      "[360,   200] loss: 0.020\n",
      "[360,   300] loss: 0.024\n",
      "[360,   400] loss: 0.023\n",
      "[360,   500] loss: 0.020\n",
      "[361,   100] loss: 0.018\n",
      "[361,   200] loss: 0.018\n",
      "[361,   300] loss: 0.021\n",
      "[361,   400] loss: 0.023\n",
      "[361,   500] loss: 0.022\n",
      "============Evaluating Epoch 360========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 40\n",
      "[362,   100] loss: 0.023\n",
      "[362,   200] loss: 0.024\n",
      "[362,   300] loss: 0.031\n",
      "[362,   400] loss: 0.028\n",
      "[362,   500] loss: 0.021\n",
      "[363,   100] loss: 0.030\n",
      "[363,   200] loss: 0.024\n",
      "[363,   300] loss: 0.023\n",
      "[363,   400] loss: 0.023\n",
      "[363,   500] loss: 0.020\n",
      "[364,   100] loss: 0.022\n",
      "[364,   200] loss: 0.023\n",
      "[364,   300] loss: 0.021\n",
      "[364,   400] loss: 0.020\n",
      "[364,   500] loss: 0.020\n",
      "[365,   100] loss: 0.026\n",
      "[365,   200] loss: 0.014\n",
      "[365,   300] loss: 0.014\n",
      "[365,   400] loss: 0.015\n",
      "[365,   500] loss: 0.014\n",
      "[366,   100] loss: 0.022\n",
      "[366,   200] loss: 0.023\n",
      "[366,   300] loss: 0.031\n",
      "[366,   400] loss: 0.025\n",
      "[366,   500] loss: 0.032\n",
      "============Evaluating Epoch 365========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 39\n",
      "[367,   100] loss: 0.029\n",
      "[367,   200] loss: 0.022\n",
      "[367,   300] loss: 0.018\n",
      "[367,   400] loss: 0.025\n",
      "[367,   500] loss: 0.026\n",
      "[368,   100] loss: 0.017\n",
      "[368,   200] loss: 0.015\n",
      "[368,   300] loss: 0.016\n",
      "[368,   400] loss: 0.020\n",
      "[368,   500] loss: 0.015\n",
      "[369,   100] loss: 0.015\n",
      "[369,   200] loss: 0.016\n",
      "[369,   300] loss: 0.026\n",
      "[369,   400] loss: 0.032\n",
      "[369,   500] loss: 0.025\n",
      "[370,   100] loss: 0.031\n",
      "[370,   200] loss: 0.028\n",
      "[370,   300] loss: 0.035\n",
      "[370,   400] loss: 0.025\n",
      "[370,   500] loss: 0.023\n",
      "[371,   100] loss: 0.018\n",
      "[371,   200] loss: 0.026\n",
      "[371,   300] loss: 0.035\n",
      "[371,   400] loss: 0.031\n",
      "[371,   500] loss: 0.023\n",
      "============Evaluating Epoch 370========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 39\n",
      "[372,   100] loss: 0.026\n",
      "[372,   200] loss: 0.024\n",
      "[372,   300] loss: 0.020\n",
      "[372,   400] loss: 0.023\n",
      "[372,   500] loss: 0.018\n",
      "[373,   100] loss: 0.019\n",
      "[373,   200] loss: 0.017\n",
      "[373,   300] loss: 0.011\n",
      "[373,   400] loss: 0.017\n",
      "[373,   500] loss: 0.016\n",
      "[374,   100] loss: 0.015\n",
      "[374,   200] loss: 0.011\n",
      "[374,   300] loss: 0.013\n",
      "[374,   400] loss: 0.018\n",
      "[374,   500] loss: 0.015\n",
      "[375,   100] loss: 0.017\n",
      "[375,   200] loss: 0.018\n",
      "[375,   300] loss: 0.019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[375,   400] loss: 0.030\n",
      "[375,   500] loss: 0.036\n",
      "[376,   100] loss: 0.033\n",
      "[376,   200] loss: 0.032\n",
      "[376,   300] loss: 0.041\n",
      "[376,   400] loss: 0.032\n",
      "[376,   500] loss: 0.032\n",
      "============Evaluating Epoch 375========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 40\n",
      "[377,   100] loss: 0.023\n",
      "[377,   200] loss: 0.025\n",
      "[377,   300] loss: 0.020\n",
      "[377,   400] loss: 0.023\n",
      "[377,   500] loss: 0.019\n",
      "[378,   100] loss: 0.017\n",
      "[378,   200] loss: 0.012\n",
      "[378,   300] loss: 0.011\n",
      "[378,   400] loss: 0.012\n",
      "[378,   500] loss: 0.013\n",
      "[379,   100] loss: 0.009\n",
      "[379,   200] loss: 0.016\n",
      "[379,   300] loss: 0.012\n",
      "[379,   400] loss: 0.016\n",
      "[379,   500] loss: 0.012\n",
      "[380,   100] loss: 0.018\n",
      "[380,   200] loss: 0.011\n",
      "[380,   300] loss: 0.015\n",
      "[380,   400] loss: 0.023\n",
      "[380,   500] loss: 0.018\n",
      "[381,   100] loss: 0.025\n",
      "[381,   200] loss: 0.027\n",
      "[381,   300] loss: 0.033\n",
      "[381,   400] loss: 0.032\n",
      "[381,   500] loss: 0.033\n",
      "============Evaluating Epoch 380========\n",
      "Train Accuracy : 97\n",
      "Test Accuracy : 39\n",
      "[382,   100] loss: 0.033\n",
      "[382,   200] loss: 0.030\n",
      "[382,   300] loss: 0.028\n",
      "[382,   400] loss: 0.025\n",
      "[382,   500] loss: 0.019\n",
      "[383,   100] loss: 0.017\n",
      "[383,   200] loss: 0.020\n",
      "[383,   300] loss: 0.015\n",
      "[383,   400] loss: 0.011\n",
      "[383,   500] loss: 0.016\n",
      "[384,   100] loss: 0.020\n",
      "[384,   200] loss: 0.019\n",
      "[384,   300] loss: 0.027\n",
      "[384,   400] loss: 0.025\n",
      "[384,   500] loss: 0.022\n",
      "[385,   100] loss: 0.024\n",
      "[385,   200] loss: 0.022\n",
      "[385,   300] loss: 0.020\n",
      "[385,   400] loss: 0.018\n",
      "[385,   500] loss: 0.018\n",
      "[386,   100] loss: 0.024\n",
      "[386,   200] loss: 0.017\n",
      "[386,   300] loss: 0.016\n",
      "[386,   400] loss: 0.013\n",
      "[386,   500] loss: 0.017\n",
      "============Evaluating Epoch 385========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 39\n",
      "[387,   100] loss: 0.016\n",
      "[387,   200] loss: 0.017\n",
      "[387,   300] loss: 0.022\n",
      "[387,   400] loss: 0.019\n",
      "[387,   500] loss: 0.025\n",
      "[388,   100] loss: 0.016\n",
      "[388,   200] loss: 0.020\n",
      "[388,   300] loss: 0.016\n",
      "[388,   400] loss: 0.021\n",
      "[388,   500] loss: 0.015\n",
      "[389,   100] loss: 0.017\n",
      "[389,   200] loss: 0.019\n",
      "[389,   300] loss: 0.015\n",
      "[389,   400] loss: 0.016\n",
      "[389,   500] loss: 0.020\n",
      "[390,   100] loss: 0.022\n",
      "[390,   200] loss: 0.024\n",
      "[390,   300] loss: 0.030\n",
      "[390,   400] loss: 0.033\n",
      "[390,   500] loss: 0.035\n",
      "[391,   100] loss: 0.030\n",
      "[391,   200] loss: 0.031\n",
      "[391,   300] loss: 0.024\n",
      "[391,   400] loss: 0.025\n",
      "[391,   500] loss: 0.028\n",
      "============Evaluating Epoch 390========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 40\n",
      "[392,   100] loss: 0.020\n",
      "[392,   200] loss: 0.023\n",
      "[392,   300] loss: 0.013\n",
      "[392,   400] loss: 0.016\n",
      "[392,   500] loss: 0.017\n",
      "[393,   100] loss: 0.016\n",
      "[393,   200] loss: 0.019\n",
      "[393,   300] loss: 0.012\n",
      "[393,   400] loss: 0.015\n",
      "[393,   500] loss: 0.016\n",
      "[394,   100] loss: 0.020\n",
      "[394,   200] loss: 0.023\n",
      "[394,   300] loss: 0.022\n",
      "[394,   400] loss: 0.016\n",
      "[394,   500] loss: 0.020\n",
      "[395,   100] loss: 0.025\n",
      "[395,   200] loss: 0.031\n",
      "[395,   300] loss: 0.031\n",
      "[395,   400] loss: 0.028\n",
      "[395,   500] loss: 0.025\n",
      "[396,   100] loss: 0.019\n",
      "[396,   200] loss: 0.017\n",
      "[396,   300] loss: 0.018\n",
      "[396,   400] loss: 0.019\n",
      "[396,   500] loss: 0.021\n",
      "============Evaluating Epoch 395========\n",
      "Train Accuracy : 98\n",
      "Test Accuracy : 40\n",
      "[397,   100] loss: 0.022\n",
      "[397,   200] loss: 0.021\n",
      "[397,   300] loss: 0.015\n",
      "[397,   400] loss: 0.016\n",
      "[397,   500] loss: 0.015\n",
      "[398,   100] loss: 0.017\n",
      "[398,   200] loss: 0.022\n",
      "[398,   300] loss: 0.015\n",
      "[398,   400] loss: 0.022\n",
      "[398,   500] loss: 0.014\n",
      "[399,   100] loss: 0.022\n",
      "[399,   200] loss: 0.024\n",
      "[399,   300] loss: 0.019\n",
      "[399,   400] loss: 0.017\n",
      "[399,   500] loss: 0.017\n",
      "[400,   100] loss: 0.015\n",
      "[400,   200] loss: 0.018\n",
      "[400,   300] loss: 0.017\n",
      "[400,   400] loss: 0.020\n",
      "[400,   500] loss: 0.022\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(shadow_model.parameters(), lr=0.0005)\n",
    "last_accuracy = 0\n",
    "\n",
    "\n",
    "for epoch in range(400):  # loop over the dataset multiple times\n",
    "    if epoch < 200:\n",
    "        continue\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "        shadow_model.train()\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # inputs = transform_train(inputs)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = shadow_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        print(f'============Evaluating Epoch {epoch}========')\n",
    "        shadow_model.eval()\n",
    "        train_accuracy = eval(train_dataloader, shadow_model)\n",
    "        test_accuracy = eval(test_dataloader, shadow_model)\n",
    "        print(f\"Train Accuracy : {train_accuracy}\")\n",
    "        print(f\"Test Accuracy : {test_accuracy}\")\n",
    "        torch.save(shadow_model.state_dict(), f\"trained_models/tinyimagenet/mobilenetv2_{epoch}_scratch.pth\")\n",
    "        \n",
    "#     if accuracy > last_accuracy:\n",
    "#         torch.save(shadow_model.state_dict(), f\"trained_models/tinyimagenet/mobilenetv2_{epoch}.pth\")\n",
    "#         last_accuracy = accuracy\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Attack Model Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Piyush\\AppData\\Local\\Temp\\ipykernel_19876\\1210369659.py:9: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attack_ip = torch.nn.functional.softmax(out[0]).cpu().detach().numpy()\n",
      "C:\\Users\\Piyush\\AppData\\Local\\Temp\\ipykernel_19876\\1210369659.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  attack_ip = torch.nn.functional.softmax(out[0]).cpu().detach().numpy()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "shadow_model.eval()\n",
    "attack_data=[]\n",
    "\n",
    "for data in train_dataset:\n",
    "    with torch.no_grad():\n",
    "        out = shadow_model(data[0].unsqueeze(0).to(device))\n",
    "        attack_ip = torch.nn.functional.softmax(out[0]).cpu().detach().numpy()\n",
    "        attack_data.append([attack_ip, 1]) # 1 for member\n",
    "        \n",
    "for data in test_dataset:\n",
    "    with torch.no_grad():\n",
    "        out = shadow_model(data[0].unsqueeze(0).to(device))\n",
    "        attack_ip = torch.nn.functional.softmax(out[0]).cpu().detach().numpy()\n",
    "        attack_data.append([attack_ip, 0]) # 0 for non-member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"attack_dataset.p\", \"wb\") as fp:\n",
    "    pickle.dump(attack_data, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Attack Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class attack_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(200, 1024)\n",
    "        self.layer_2 = nn.Linear(1024, 512)\n",
    "        self.layer_3 = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.layer_3(x)\n",
    "        return x\n",
    "    \n",
    "attack_model = attack_model().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack_model = models.mobilenet_v2(pretrained=True)\n",
    "\n",
    "# attack_model.classifier[1] = torch.nn.Linear(attack_model.classifier[1].in_features, 2)\n",
    "# attack_model = attack_model.to(device)\n",
    "# print(attack_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attack_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader_attack = torch.utils.data.DataLoader(\n",
    "    attack_data, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.692\n",
      "[1,   200] loss: 0.688\n",
      "[1,   300] loss: 0.683\n",
      "[1,   400] loss: 0.677\n",
      "[1,   500] loss: 0.670\n",
      "[1,   600] loss: 0.658\n",
      "[1,   700] loss: 0.650\n",
      "[2,   100] loss: 0.628\n",
      "[2,   200] loss: 0.622\n",
      "[2,   300] loss: 0.614\n",
      "[2,   400] loss: 0.607\n",
      "[2,   500] loss: 0.611\n",
      "[2,   600] loss: 0.594\n",
      "[2,   700] loss: 0.588\n",
      "[3,   100] loss: 0.582\n",
      "[3,   200] loss: 0.584\n",
      "[3,   300] loss: 0.576\n",
      "[3,   400] loss: 0.588\n",
      "[3,   500] loss: 0.580\n",
      "[3,   600] loss: 0.582\n",
      "[3,   700] loss: 0.580\n",
      "[4,   100] loss: 0.578\n",
      "[4,   200] loss: 0.567\n",
      "[4,   300] loss: 0.566\n",
      "[4,   400] loss: 0.572\n",
      "[4,   500] loss: 0.573\n",
      "[4,   600] loss: 0.572\n",
      "[4,   700] loss: 0.571\n",
      "[5,   100] loss: 0.575\n",
      "[5,   200] loss: 0.562\n",
      "[5,   300] loss: 0.578\n",
      "[5,   400] loss: 0.561\n",
      "[5,   500] loss: 0.562\n",
      "[5,   600] loss: 0.569\n",
      "[5,   700] loss: 0.556\n",
      "[6,   100] loss: 0.560\n",
      "[6,   200] loss: 0.556\n",
      "[6,   300] loss: 0.567\n",
      "[6,   400] loss: 0.559\n",
      "[6,   500] loss: 0.564\n",
      "[6,   600] loss: 0.565\n",
      "[6,   700] loss: 0.567\n",
      "[7,   100] loss: 0.573\n",
      "[7,   200] loss: 0.563\n",
      "[7,   300] loss: 0.572\n",
      "[7,   400] loss: 0.552\n",
      "[7,   500] loss: 0.554\n",
      "[7,   600] loss: 0.562\n",
      "[7,   700] loss: 0.563\n",
      "[8,   100] loss: 0.565\n",
      "[8,   200] loss: 0.547\n",
      "[8,   300] loss: 0.557\n",
      "[8,   400] loss: 0.557\n",
      "[8,   500] loss: 0.574\n",
      "[8,   600] loss: 0.567\n",
      "[8,   700] loss: 0.555\n",
      "[9,   100] loss: 0.557\n",
      "[9,   200] loss: 0.556\n",
      "[9,   300] loss: 0.555\n",
      "[9,   400] loss: 0.560\n",
      "[9,   500] loss: 0.563\n",
      "[9,   600] loss: 0.556\n",
      "[9,   700] loss: 0.555\n",
      "[10,   100] loss: 0.548\n",
      "[10,   200] loss: 0.563\n",
      "[10,   300] loss: 0.563\n",
      "[10,   400] loss: 0.559\n",
      "[10,   500] loss: 0.555\n",
      "[10,   600] loss: 0.568\n",
      "[10,   700] loss: 0.545\n",
      "[11,   100] loss: 0.554\n",
      "[11,   200] loss: 0.559\n",
      "[11,   300] loss: 0.561\n",
      "[11,   400] loss: 0.554\n",
      "[11,   500] loss: 0.553\n",
      "[11,   600] loss: 0.550\n",
      "[11,   700] loss: 0.556\n",
      "[12,   100] loss: 0.562\n",
      "[12,   200] loss: 0.549\n",
      "[12,   300] loss: 0.550\n",
      "[12,   400] loss: 0.549\n",
      "[12,   500] loss: 0.556\n",
      "[12,   600] loss: 0.552\n",
      "[12,   700] loss: 0.557\n",
      "[13,   100] loss: 0.547\n",
      "[13,   200] loss: 0.544\n",
      "[13,   300] loss: 0.548\n",
      "[13,   400] loss: 0.562\n",
      "[13,   500] loss: 0.552\n",
      "[13,   600] loss: 0.549\n",
      "[13,   700] loss: 0.552\n",
      "[14,   100] loss: 0.549\n",
      "[14,   200] loss: 0.551\n",
      "[14,   300] loss: 0.558\n",
      "[14,   400] loss: 0.538\n",
      "[14,   500] loss: 0.562\n",
      "[14,   600] loss: 0.551\n",
      "[14,   700] loss: 0.549\n",
      "[15,   100] loss: 0.546\n",
      "[15,   200] loss: 0.547\n",
      "[15,   300] loss: 0.553\n",
      "[15,   400] loss: 0.548\n",
      "[15,   500] loss: 0.547\n",
      "[15,   600] loss: 0.561\n",
      "[15,   700] loss: 0.540\n",
      "[16,   100] loss: 0.545\n",
      "[16,   200] loss: 0.546\n",
      "[16,   300] loss: 0.546\n",
      "[16,   400] loss: 0.549\n",
      "[16,   500] loss: 0.553\n",
      "[16,   600] loss: 0.548\n",
      "[16,   700] loss: 0.554\n",
      "[17,   100] loss: 0.543\n",
      "[17,   200] loss: 0.550\n",
      "[17,   300] loss: 0.539\n",
      "[17,   400] loss: 0.546\n",
      "[17,   500] loss: 0.548\n",
      "[17,   600] loss: 0.557\n",
      "[17,   700] loss: 0.548\n",
      "[18,   100] loss: 0.548\n",
      "[18,   200] loss: 0.545\n",
      "[18,   300] loss: 0.544\n",
      "[18,   400] loss: 0.552\n",
      "[18,   500] loss: 0.541\n",
      "[18,   600] loss: 0.545\n",
      "[18,   700] loss: 0.541\n",
      "[19,   100] loss: 0.541\n",
      "[19,   200] loss: 0.523\n",
      "[19,   300] loss: 0.540\n",
      "[19,   400] loss: 0.546\n",
      "[19,   500] loss: 0.550\n",
      "[19,   600] loss: 0.545\n",
      "[19,   700] loss: 0.550\n",
      "[20,   100] loss: 0.542\n",
      "[20,   200] loss: 0.549\n",
      "[20,   300] loss: 0.535\n",
      "[20,   400] loss: 0.547\n",
      "[20,   500] loss: 0.552\n",
      "[20,   600] loss: 0.544\n",
      "[20,   700] loss: 0.541\n",
      "[21,   100] loss: 0.536\n",
      "[21,   200] loss: 0.539\n",
      "[21,   300] loss: 0.550\n",
      "[21,   400] loss: 0.538\n",
      "[21,   500] loss: 0.545\n",
      "[21,   600] loss: 0.542\n",
      "[21,   700] loss: 0.539\n",
      "[22,   100] loss: 0.541\n",
      "[22,   200] loss: 0.529\n",
      "[22,   300] loss: 0.540\n",
      "[22,   400] loss: 0.550\n",
      "[22,   500] loss: 0.537\n",
      "[22,   600] loss: 0.555\n",
      "[22,   700] loss: 0.542\n",
      "[23,   100] loss: 0.537\n",
      "[23,   200] loss: 0.541\n",
      "[23,   300] loss: 0.551\n",
      "[23,   400] loss: 0.534\n",
      "[23,   500] loss: 0.538\n",
      "[23,   600] loss: 0.541\n",
      "[23,   700] loss: 0.543\n",
      "[24,   100] loss: 0.541\n",
      "[24,   200] loss: 0.527\n",
      "[24,   300] loss: 0.542\n",
      "[24,   400] loss: 0.531\n",
      "[24,   500] loss: 0.543\n",
      "[24,   600] loss: 0.548\n",
      "[24,   700] loss: 0.540\n",
      "[25,   100] loss: 0.549\n",
      "[25,   200] loss: 0.535\n",
      "[25,   300] loss: 0.536\n",
      "[25,   400] loss: 0.540\n",
      "[25,   500] loss: 0.538\n",
      "[25,   600] loss: 0.538\n",
      "[25,   700] loss: 0.539\n",
      "[26,   100] loss: 0.542\n",
      "[26,   200] loss: 0.549\n",
      "[26,   300] loss: 0.539\n",
      "[26,   400] loss: 0.527\n",
      "[26,   500] loss: 0.536\n",
      "[26,   600] loss: 0.545\n",
      "[26,   700] loss: 0.534\n",
      "[27,   100] loss: 0.542\n",
      "[27,   200] loss: 0.538\n",
      "[27,   300] loss: 0.532\n",
      "[27,   400] loss: 0.540\n",
      "[27,   500] loss: 0.541\n",
      "[27,   600] loss: 0.537\n",
      "[27,   700] loss: 0.537\n",
      "[28,   100] loss: 0.541\n",
      "[28,   200] loss: 0.552\n",
      "[28,   300] loss: 0.546\n",
      "[28,   400] loss: 0.533\n",
      "[28,   500] loss: 0.536\n",
      "[28,   600] loss: 0.529\n",
      "[28,   700] loss: 0.523\n",
      "[29,   100] loss: 0.533\n",
      "[29,   200] loss: 0.541\n",
      "[29,   300] loss: 0.541\n",
      "[29,   400] loss: 0.547\n",
      "[29,   500] loss: 0.523\n",
      "[29,   600] loss: 0.533\n",
      "[29,   700] loss: 0.529\n",
      "[30,   100] loss: 0.528\n",
      "[30,   200] loss: 0.536\n",
      "[30,   300] loss: 0.531\n",
      "[30,   400] loss: 0.541\n",
      "[30,   500] loss: 0.553\n",
      "[30,   600] loss: 0.532\n",
      "[30,   700] loss: 0.526\n",
      "[31,   100] loss: 0.547\n",
      "[31,   200] loss: 0.524\n",
      "[31,   300] loss: 0.535\n",
      "[31,   400] loss: 0.536\n",
      "[31,   500] loss: 0.540\n",
      "[31,   600] loss: 0.539\n",
      "[31,   700] loss: 0.532\n",
      "[32,   100] loss: 0.525\n",
      "[32,   200] loss: 0.536\n",
      "[32,   300] loss: 0.531\n",
      "[32,   400] loss: 0.537\n",
      "[32,   500] loss: 0.535\n",
      "[32,   600] loss: 0.532\n",
      "[32,   700] loss: 0.537\n",
      "[33,   100] loss: 0.528\n",
      "[33,   200] loss: 0.528\n",
      "[33,   300] loss: 0.542\n",
      "[33,   400] loss: 0.533\n",
      "[33,   500] loss: 0.541\n",
      "[33,   600] loss: 0.527\n",
      "[33,   700] loss: 0.542\n",
      "[34,   100] loss: 0.527\n",
      "[34,   200] loss: 0.528\n",
      "[34,   300] loss: 0.542\n",
      "[34,   400] loss: 0.518\n",
      "[34,   500] loss: 0.533\n",
      "[34,   600] loss: 0.529\n",
      "[34,   700] loss: 0.548\n",
      "[35,   100] loss: 0.528\n",
      "[35,   200] loss: 0.530\n",
      "[35,   300] loss: 0.520\n",
      "[35,   400] loss: 0.541\n",
      "[35,   500] loss: 0.534\n",
      "[35,   600] loss: 0.533\n",
      "[35,   700] loss: 0.526\n",
      "[36,   100] loss: 0.529\n",
      "[36,   200] loss: 0.533\n",
      "[36,   300] loss: 0.532\n",
      "[36,   400] loss: 0.525\n",
      "[36,   500] loss: 0.533\n",
      "[36,   600] loss: 0.528\n",
      "[36,   700] loss: 0.542\n",
      "[37,   100] loss: 0.531\n",
      "[37,   200] loss: 0.526\n",
      "[37,   300] loss: 0.524\n",
      "[37,   400] loss: 0.534\n",
      "[37,   500] loss: 0.538\n",
      "[37,   600] loss: 0.537\n",
      "[37,   700] loss: 0.527\n",
      "[38,   100] loss: 0.519\n",
      "[38,   200] loss: 0.531\n",
      "[38,   300] loss: 0.527\n",
      "[38,   400] loss: 0.539\n",
      "[38,   500] loss: 0.536\n",
      "[38,   600] loss: 0.523\n",
      "[38,   700] loss: 0.536\n",
      "[39,   100] loss: 0.530\n",
      "[39,   200] loss: 0.530\n",
      "[39,   300] loss: 0.540\n",
      "[39,   400] loss: 0.517\n",
      "[39,   500] loss: 0.534\n",
      "[39,   600] loss: 0.533\n",
      "[39,   700] loss: 0.523\n",
      "[40,   100] loss: 0.535\n",
      "[40,   200] loss: 0.528\n",
      "[40,   300] loss: 0.517\n",
      "[40,   400] loss: 0.536\n",
      "[40,   500] loss: 0.540\n",
      "[40,   600] loss: 0.520\n",
      "[40,   700] loss: 0.531\n",
      "[41,   100] loss: 0.530\n",
      "[41,   200] loss: 0.517\n",
      "[41,   300] loss: 0.516\n",
      "[41,   400] loss: 0.531\n",
      "[41,   500] loss: 0.542\n",
      "[41,   600] loss: 0.536\n",
      "[41,   700] loss: 0.525\n",
      "[42,   100] loss: 0.528\n",
      "[42,   200] loss: 0.530\n",
      "[42,   300] loss: 0.532\n",
      "[42,   400] loss: 0.530\n",
      "[42,   500] loss: 0.523\n",
      "[42,   600] loss: 0.537\n",
      "[42,   700] loss: 0.522\n",
      "[43,   100] loss: 0.524\n",
      "[43,   200] loss: 0.514\n",
      "[43,   300] loss: 0.526\n",
      "[43,   400] loss: 0.534\n",
      "[43,   500] loss: 0.523\n",
      "[43,   600] loss: 0.530\n",
      "[43,   700] loss: 0.533\n",
      "[44,   100] loss: 0.521\n",
      "[44,   200] loss: 0.538\n",
      "[44,   300] loss: 0.529\n",
      "[44,   400] loss: 0.522\n",
      "[44,   500] loss: 0.524\n",
      "[44,   600] loss: 0.528\n",
      "[44,   700] loss: 0.527\n",
      "[45,   100] loss: 0.521\n",
      "[45,   200] loss: 0.530\n",
      "[45,   300] loss: 0.517\n",
      "[45,   400] loss: 0.526\n",
      "[45,   500] loss: 0.529\n",
      "[45,   600] loss: 0.522\n",
      "[45,   700] loss: 0.530\n",
      "[46,   100] loss: 0.531\n",
      "[46,   200] loss: 0.524\n",
      "[46,   300] loss: 0.519\n",
      "[46,   400] loss: 0.516\n",
      "[46,   500] loss: 0.543\n",
      "[46,   600] loss: 0.526\n",
      "[46,   700] loss: 0.523\n",
      "[47,   100] loss: 0.528\n",
      "[47,   200] loss: 0.517\n",
      "[47,   300] loss: 0.519\n",
      "[47,   400] loss: 0.530\n",
      "[47,   500] loss: 0.534\n",
      "[47,   600] loss: 0.520\n",
      "[47,   700] loss: 0.530\n",
      "[48,   100] loss: 0.529\n",
      "[48,   200] loss: 0.517\n",
      "[48,   300] loss: 0.528\n",
      "[48,   400] loss: 0.529\n",
      "[48,   500] loss: 0.527\n",
      "[48,   600] loss: 0.527\n",
      "[48,   700] loss: 0.522\n",
      "[49,   100] loss: 0.523\n",
      "[49,   200] loss: 0.518\n",
      "[49,   300] loss: 0.523\n",
      "[49,   400] loss: 0.528\n",
      "[49,   500] loss: 0.523\n",
      "[49,   600] loss: 0.530\n",
      "[49,   700] loss: 0.527\n",
      "[50,   100] loss: 0.520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50,   200] loss: 0.534\n",
      "[50,   300] loss: 0.521\n",
      "[50,   400] loss: 0.518\n",
      "[50,   500] loss: 0.525\n",
      "[50,   600] loss: 0.529\n",
      "[50,   700] loss: 0.515\n",
      "[51,   100] loss: 0.522\n",
      "[51,   200] loss: 0.515\n",
      "[51,   300] loss: 0.533\n",
      "[51,   400] loss: 0.523\n",
      "[51,   500] loss: 0.523\n",
      "[51,   600] loss: 0.522\n",
      "[51,   700] loss: 0.516\n",
      "[52,   100] loss: 0.528\n",
      "[52,   200] loss: 0.516\n",
      "[52,   300] loss: 0.519\n",
      "[52,   400] loss: 0.513\n",
      "[52,   500] loss: 0.528\n",
      "[52,   600] loss: 0.526\n",
      "[52,   700] loss: 0.527\n",
      "[53,   100] loss: 0.516\n",
      "[53,   200] loss: 0.516\n",
      "[53,   300] loss: 0.524\n",
      "[53,   400] loss: 0.522\n",
      "[53,   500] loss: 0.525\n",
      "[53,   600] loss: 0.528\n",
      "[53,   700] loss: 0.521\n",
      "[54,   100] loss: 0.511\n",
      "[54,   200] loss: 0.514\n",
      "[54,   300] loss: 0.535\n",
      "[54,   400] loss: 0.524\n",
      "[54,   500] loss: 0.511\n",
      "[54,   600] loss: 0.520\n",
      "[54,   700] loss: 0.527\n",
      "[55,   100] loss: 0.509\n",
      "[55,   200] loss: 0.518\n",
      "[55,   300] loss: 0.520\n",
      "[55,   400] loss: 0.531\n",
      "[55,   500] loss: 0.537\n",
      "[55,   600] loss: 0.519\n",
      "[55,   700] loss: 0.511\n",
      "[56,   100] loss: 0.518\n",
      "[56,   200] loss: 0.531\n",
      "[56,   300] loss: 0.515\n",
      "[56,   400] loss: 0.516\n",
      "[56,   500] loss: 0.521\n",
      "[56,   600] loss: 0.521\n",
      "[56,   700] loss: 0.517\n",
      "[57,   100] loss: 0.511\n",
      "[57,   200] loss: 0.524\n",
      "[57,   300] loss: 0.528\n",
      "[57,   400] loss: 0.518\n",
      "[57,   500] loss: 0.519\n",
      "[57,   600] loss: 0.523\n",
      "[57,   700] loss: 0.515\n",
      "[58,   100] loss: 0.512\n",
      "[58,   200] loss: 0.517\n",
      "[58,   300] loss: 0.526\n",
      "[58,   400] loss: 0.525\n",
      "[58,   500] loss: 0.520\n",
      "[58,   600] loss: 0.516\n",
      "[58,   700] loss: 0.516\n",
      "[59,   100] loss: 0.513\n",
      "[59,   200] loss: 0.528\n",
      "[59,   300] loss: 0.513\n",
      "[59,   400] loss: 0.523\n",
      "[59,   500] loss: 0.527\n",
      "[59,   600] loss: 0.510\n",
      "[59,   700] loss: 0.518\n",
      "[60,   100] loss: 0.525\n",
      "[60,   200] loss: 0.513\n",
      "[60,   300] loss: 0.511\n",
      "[60,   400] loss: 0.514\n",
      "[60,   500] loss: 0.513\n",
      "[60,   600] loss: 0.524\n",
      "[60,   700] loss: 0.520\n",
      "[61,   100] loss: 0.514\n",
      "[61,   200] loss: 0.509\n",
      "[61,   300] loss: 0.526\n",
      "[61,   400] loss: 0.503\n",
      "[61,   500] loss: 0.508\n",
      "[61,   600] loss: 0.526\n",
      "[61,   700] loss: 0.535\n",
      "[62,   100] loss: 0.518\n",
      "[62,   200] loss: 0.527\n",
      "[62,   300] loss: 0.513\n",
      "[62,   400] loss: 0.513\n",
      "[62,   500] loss: 0.518\n",
      "[62,   600] loss: 0.517\n",
      "[62,   700] loss: 0.515\n",
      "[63,   100] loss: 0.513\n",
      "[63,   200] loss: 0.523\n",
      "[63,   300] loss: 0.517\n",
      "[63,   400] loss: 0.524\n",
      "[63,   500] loss: 0.506\n",
      "[63,   600] loss: 0.510\n",
      "[63,   700] loss: 0.528\n",
      "[64,   100] loss: 0.526\n",
      "[64,   200] loss: 0.527\n",
      "[64,   300] loss: 0.520\n",
      "[64,   400] loss: 0.513\n",
      "[64,   500] loss: 0.513\n",
      "[64,   600] loss: 0.503\n",
      "[64,   700] loss: 0.515\n",
      "[65,   100] loss: 0.519\n",
      "[65,   200] loss: 0.519\n",
      "[65,   300] loss: 0.498\n",
      "[65,   400] loss: 0.512\n",
      "[65,   500] loss: 0.524\n",
      "[65,   600] loss: 0.522\n",
      "[65,   700] loss: 0.522\n",
      "[66,   100] loss: 0.513\n",
      "[66,   200] loss: 0.523\n",
      "[66,   300] loss: 0.513\n",
      "[66,   400] loss: 0.515\n",
      "[66,   500] loss: 0.514\n",
      "[66,   600] loss: 0.514\n",
      "[66,   700] loss: 0.512\n",
      "[67,   100] loss: 0.513\n",
      "[67,   200] loss: 0.515\n",
      "[67,   300] loss: 0.511\n",
      "[67,   400] loss: 0.516\n",
      "[67,   500] loss: 0.516\n",
      "[67,   600] loss: 0.512\n",
      "[67,   700] loss: 0.521\n",
      "[68,   100] loss: 0.510\n",
      "[68,   200] loss: 0.503\n",
      "[68,   300] loss: 0.519\n",
      "[68,   400] loss: 0.514\n",
      "[68,   500] loss: 0.527\n",
      "[68,   600] loss: 0.511\n",
      "[68,   700] loss: 0.526\n",
      "[69,   100] loss: 0.515\n",
      "[69,   200] loss: 0.511\n",
      "[69,   300] loss: 0.516\n",
      "[69,   400] loss: 0.518\n",
      "[69,   500] loss: 0.511\n",
      "[69,   600] loss: 0.511\n",
      "[69,   700] loss: 0.516\n",
      "[70,   100] loss: 0.521\n",
      "[70,   200] loss: 0.518\n",
      "[70,   300] loss: 0.513\n",
      "[70,   400] loss: 0.512\n",
      "[70,   500] loss: 0.513\n",
      "[70,   600] loss: 0.514\n",
      "[70,   700] loss: 0.514\n",
      "[71,   100] loss: 0.507\n",
      "[71,   200] loss: 0.531\n",
      "[71,   300] loss: 0.514\n",
      "[71,   400] loss: 0.510\n",
      "[71,   500] loss: 0.513\n",
      "[71,   600] loss: 0.510\n",
      "[71,   700] loss: 0.500\n",
      "[72,   100] loss: 0.510\n",
      "[72,   200] loss: 0.511\n",
      "[72,   300] loss: 0.523\n",
      "[72,   400] loss: 0.494\n",
      "[72,   500] loss: 0.500\n",
      "[72,   600] loss: 0.524\n",
      "[72,   700] loss: 0.528\n",
      "[73,   100] loss: 0.508\n",
      "[73,   200] loss: 0.512\n",
      "[73,   300] loss: 0.511\n",
      "[73,   400] loss: 0.516\n",
      "[73,   500] loss: 0.520\n",
      "[73,   600] loss: 0.512\n",
      "[73,   700] loss: 0.514\n",
      "[74,   100] loss: 0.519\n",
      "[74,   200] loss: 0.519\n",
      "[74,   300] loss: 0.517\n",
      "[74,   400] loss: 0.502\n",
      "[74,   500] loss: 0.499\n",
      "[74,   600] loss: 0.505\n",
      "[74,   700] loss: 0.513\n",
      "[75,   100] loss: 0.507\n",
      "[75,   200] loss: 0.522\n",
      "[75,   300] loss: 0.519\n",
      "[75,   400] loss: 0.508\n",
      "[75,   500] loss: 0.506\n",
      "[75,   600] loss: 0.502\n",
      "[75,   700] loss: 0.515\n",
      "[76,   100] loss: 0.515\n",
      "[76,   200] loss: 0.509\n",
      "[76,   300] loss: 0.502\n",
      "[76,   400] loss: 0.515\n",
      "[76,   500] loss: 0.514\n",
      "[76,   600] loss: 0.517\n",
      "[76,   700] loss: 0.502\n",
      "[77,   100] loss: 0.501\n",
      "[77,   200] loss: 0.511\n",
      "[77,   300] loss: 0.507\n",
      "[77,   400] loss: 0.509\n",
      "[77,   500] loss: 0.516\n",
      "[77,   600] loss: 0.508\n",
      "[77,   700] loss: 0.511\n",
      "[78,   100] loss: 0.509\n",
      "[78,   200] loss: 0.517\n",
      "[78,   300] loss: 0.503\n",
      "[78,   400] loss: 0.521\n",
      "[78,   500] loss: 0.501\n",
      "[78,   600] loss: 0.517\n",
      "[78,   700] loss: 0.500\n",
      "[79,   100] loss: 0.508\n",
      "[79,   200] loss: 0.522\n",
      "[79,   300] loss: 0.510\n",
      "[79,   400] loss: 0.522\n",
      "[79,   500] loss: 0.500\n",
      "[79,   600] loss: 0.500\n",
      "[79,   700] loss: 0.494\n",
      "[80,   100] loss: 0.516\n",
      "[80,   200] loss: 0.499\n",
      "[80,   300] loss: 0.504\n",
      "[80,   400] loss: 0.512\n",
      "[80,   500] loss: 0.508\n",
      "[80,   600] loss: 0.507\n",
      "[80,   700] loss: 0.512\n",
      "[81,   100] loss: 0.506\n",
      "[81,   200] loss: 0.502\n",
      "[81,   300] loss: 0.510\n",
      "[81,   400] loss: 0.520\n",
      "[81,   500] loss: 0.514\n",
      "[81,   600] loss: 0.506\n",
      "[81,   700] loss: 0.503\n",
      "[82,   100] loss: 0.491\n",
      "[82,   200] loss: 0.505\n",
      "[82,   300] loss: 0.512\n",
      "[82,   400] loss: 0.500\n",
      "[82,   500] loss: 0.515\n",
      "[82,   600] loss: 0.513\n",
      "[82,   700] loss: 0.509\n",
      "[83,   100] loss: 0.507\n",
      "[83,   200] loss: 0.513\n",
      "[83,   300] loss: 0.504\n",
      "[83,   400] loss: 0.510\n",
      "[83,   500] loss: 0.508\n",
      "[83,   600] loss: 0.517\n",
      "[83,   700] loss: 0.501\n",
      "[84,   100] loss: 0.503\n",
      "[84,   200] loss: 0.493\n",
      "[84,   300] loss: 0.512\n",
      "[84,   400] loss: 0.511\n",
      "[84,   500] loss: 0.500\n",
      "[84,   600] loss: 0.510\n",
      "[84,   700] loss: 0.513\n",
      "[85,   100] loss: 0.493\n",
      "[85,   200] loss: 0.510\n",
      "[85,   300] loss: 0.503\n",
      "[85,   400] loss: 0.514\n",
      "[85,   500] loss: 0.507\n",
      "[85,   600] loss: 0.493\n",
      "[85,   700] loss: 0.515\n",
      "[86,   100] loss: 0.508\n",
      "[86,   200] loss: 0.496\n",
      "[86,   300] loss: 0.512\n",
      "[86,   400] loss: 0.495\n",
      "[86,   500] loss: 0.509\n",
      "[86,   600] loss: 0.504\n",
      "[86,   700] loss: 0.506\n",
      "[87,   100] loss: 0.506\n",
      "[87,   200] loss: 0.507\n",
      "[87,   300] loss: 0.495\n",
      "[87,   400] loss: 0.513\n",
      "[87,   500] loss: 0.494\n",
      "[87,   600] loss: 0.509\n",
      "[87,   700] loss: 0.519\n",
      "[88,   100] loss: 0.502\n",
      "[88,   200] loss: 0.500\n",
      "[88,   300] loss: 0.496\n",
      "[88,   400] loss: 0.505\n",
      "[88,   500] loss: 0.515\n",
      "[88,   600] loss: 0.501\n",
      "[88,   700] loss: 0.506\n",
      "[89,   100] loss: 0.498\n",
      "[89,   200] loss: 0.509\n",
      "[89,   300] loss: 0.498\n",
      "[89,   400] loss: 0.507\n",
      "[89,   500] loss: 0.513\n",
      "[89,   600] loss: 0.498\n",
      "[89,   700] loss: 0.502\n",
      "[90,   100] loss: 0.492\n",
      "[90,   200] loss: 0.509\n",
      "[90,   300] loss: 0.496\n",
      "[90,   400] loss: 0.499\n",
      "[90,   500] loss: 0.506\n",
      "[90,   600] loss: 0.509\n",
      "[90,   700] loss: 0.509\n",
      "[91,   100] loss: 0.487\n",
      "[91,   200] loss: 0.500\n",
      "[91,   300] loss: 0.502\n",
      "[91,   400] loss: 0.494\n",
      "[91,   500] loss: 0.504\n",
      "[91,   600] loss: 0.511\n",
      "[91,   700] loss: 0.510\n",
      "[92,   100] loss: 0.509\n",
      "[92,   200] loss: 0.506\n",
      "[92,   300] loss: 0.504\n",
      "[92,   400] loss: 0.501\n",
      "[92,   500] loss: 0.495\n",
      "[92,   600] loss: 0.501\n",
      "[92,   700] loss: 0.500\n",
      "[93,   100] loss: 0.498\n",
      "[93,   200] loss: 0.503\n",
      "[93,   300] loss: 0.490\n",
      "[93,   400] loss: 0.507\n",
      "[93,   500] loss: 0.488\n",
      "[93,   600] loss: 0.507\n",
      "[93,   700] loss: 0.506\n",
      "[94,   100] loss: 0.501\n",
      "[94,   200] loss: 0.498\n",
      "[94,   300] loss: 0.504\n",
      "[94,   400] loss: 0.496\n",
      "[94,   500] loss: 0.486\n",
      "[94,   600] loss: 0.499\n",
      "[94,   700] loss: 0.504\n",
      "[95,   100] loss: 0.501\n",
      "[95,   200] loss: 0.501\n",
      "[95,   300] loss: 0.501\n",
      "[95,   400] loss: 0.501\n",
      "[95,   500] loss: 0.500\n",
      "[95,   600] loss: 0.499\n",
      "[95,   700] loss: 0.492\n",
      "[96,   100] loss: 0.498\n",
      "[96,   200] loss: 0.508\n",
      "[96,   300] loss: 0.488\n",
      "[96,   400] loss: 0.495\n",
      "[96,   500] loss: 0.496\n",
      "[96,   600] loss: 0.500\n",
      "[96,   700] loss: 0.502\n",
      "[97,   100] loss: 0.488\n",
      "[97,   200] loss: 0.496\n",
      "[97,   300] loss: 0.495\n",
      "[97,   400] loss: 0.497\n",
      "[97,   500] loss: 0.500\n",
      "[97,   600] loss: 0.502\n",
      "[97,   700] loss: 0.505\n",
      "[98,   100] loss: 0.495\n",
      "[98,   200] loss: 0.498\n",
      "[98,   300] loss: 0.495\n",
      "[98,   400] loss: 0.499\n",
      "[98,   500] loss: 0.498\n",
      "[98,   600] loss: 0.501\n",
      "[98,   700] loss: 0.497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99,   100] loss: 0.500\n",
      "[99,   200] loss: 0.498\n",
      "[99,   300] loss: 0.500\n",
      "[99,   400] loss: 0.500\n",
      "[99,   500] loss: 0.499\n",
      "[99,   600] loss: 0.490\n",
      "[99,   700] loss: 0.497\n",
      "[100,   100] loss: 0.493\n",
      "[100,   200] loss: 0.484\n",
      "[100,   300] loss: 0.507\n",
      "[100,   400] loss: 0.508\n",
      "[100,   500] loss: 0.490\n",
      "[100,   600] loss: 0.493\n",
      "[100,   700] loss: 0.495\n"
     ]
    }
   ],
   "source": [
    "weights = torch.tensor([3.34, 1.43]).to(device) ## to deal with class imbalance\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "optimizer = optim.Adam(attack_model.parameters(), lr=0.00005)\n",
    "last_accuracy = 0\n",
    "\n",
    "\n",
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader_attack, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        \n",
    "        attack_model.train()\n",
    "        \n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = attack_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 100:.3f}')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "#         print(f'============Evaluating Epoch {epoch}========')\n",
    "#         shadow_model.eval()\n",
    "#         train_accuracy = eval(train_dataloader, attack_model)\n",
    "#         test_accuracy = eval(test_dataloader, attack_model)\n",
    "#         print(f\"Train Accuracy : {train_accuracy}\")\n",
    "#         print(f\"Test Accuracy : {test_accuracy}\")\n",
    "        torch.save(attack_model.state_dict(), f\"trained_models/tinyimagenet/mobilenetv2_{epoch}_attack.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attack_model.eval()\n",
    "eval(train_dataloader_attack, attack_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ##Test Attack model on eval.p\n",
    "# attack_model = attack_model().to(device)\n",
    "attack_model_path = \"trained_models/tinyimagenet/mobilenetv2_50_attack.pth\"\n",
    "state_dict = torch.load(attack_model_path, map_location=device)\n",
    "attack_model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Gokul's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2dNormActivation(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "          (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "          (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "          (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "          (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "          (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "          (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2dNormActivation(\n",
      "          (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (1): Conv2dNormActivation(\n",
      "          (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "          (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU6(inplace=True)\n",
      "        )\n",
      "        (2): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (18): Conv2dNormActivation(\n",
      "      (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.2, inplace=False)\n",
      "    (1): Linear(in_features=1280, out_features=200, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "MODEL_PATH = 'models/mobilenetv2_tinyimagenet.pth'\n",
    "# Change the MODEL_PATH to your local model path\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "target_model = models.mobilenet_v2(num_classes=200).to(device)\n",
    "# Change num_classes to 200 when you use the Tiny ImageNet dataset\n",
    "\n",
    "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "target_model.load_state_dict(state_dict['net'])\n",
    "\n",
    "print(target_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Piyush\\AppData\\Local\\Temp\\ipykernel_19876\\2429775316.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  target_op = torch.nn.functional.softmax(out[0])\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = 'pickle/tinyimagenet/mobilenetv2/eval.p'\n",
    "\n",
    "with open(DATA_PATH, \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "    \n",
    "import numpy as np\n",
    "## Get logits from target model and give as input to attack model ##\n",
    "\n",
    "prediction = []\n",
    "ground_truth  = []\n",
    "\n",
    "target_model.eval()\n",
    "for i in range(0,len(dataset)):\n",
    "    out = target_model(dataset[i][0].unsqueeze(0).to(device))\n",
    "    target_op = torch.nn.functional.softmax(out[0])\n",
    "    \n",
    "    prediction.append(torch.argmax(attack_model(target_op.unsqueeze(0).to(device))).cpu().detach().item())\n",
    "    ground_truth.append(dataset[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of attack model on eval dataset: 0.875\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Accuracy of attack model on eval dataset:\", accuracy_score(ground_truth, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'pickle/tinyimagenet/mobilenetv2/test.p'\n",
    "\n",
    "with open(DATA_PATH, \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "import numpy as np\n",
    "## Get logits from target model and give as input to attack model ##\n",
    "\n",
    "prediction = []\n",
    "target_model.eval()\n",
    "for i in range(0,len(dataset)):\n",
    "    out = target_model(dataset[i][0].unsqueeze(0).to(device))\n",
    "    target_op = torch.nn.functional.softmax(out[0])\n",
    "    \n",
    "    prediction.append(torch.argmax(attack_model(target_op.unsqueeze(0).to(device))).cpu().detach().item())\n",
    "\n",
    "np.save('results/task3_mobilenetv2_tinyimagenet.npy', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Attack model on eval.p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_attack(test_loader, shadow_model, attack_model):\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     shadow_model.eval()\n",
    "#     attack_model.eval()\n",
    "#     # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "#     with torch.no_grad():\n",
    "#         for data in test_loader:\n",
    "#             images, labels, member = data\n",
    "#             images = images.to(device)\n",
    "#             member = member.to(device)\n",
    "            \n",
    "#             out = shadow_model(images.to(device))\n",
    "#             attack_ip = torch.nn.functional.softmax(out).cpu().detach().numpy()\n",
    "# #             print(attack_ip.shape)\n",
    "# #             print(attack_ip[0])\n",
    "#             sorted_attack_ip = np.sort(attack_ip,1)[:,:10]\n",
    "# #             print(sorted_attack_ip[0])\n",
    "#             sorted_attack_ip = torch.tensor(sorted_attack_ip.copy()).to(device)\n",
    "# #             print(sorted_attack_ip.shape)\n",
    "#             outputs = attack_model(sorted_attack_ip)\n",
    "#             # the class with the highest energy is what we choose as prediction\n",
    "#             _, predicted = torch.max(outputs.data, 1)\n",
    "#             total += member.size(0)\n",
    "#             correct += (predicted == member).sum().item()\n",
    "#     accuracy = 100 * correct // total\n",
    "\n",
    "# #     print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
    "#     return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = 'pickle/tinyimagenet/mobilenetv2/eval.p'\n",
    "# # Change the DATA_PATH to your local pickle file path\n",
    "\n",
    "# device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# with open(DATA_PATH, \"rb\") as f:\n",
    "#     dataset = pickle.load(f)\n",
    "\n",
    "\n",
    "# print(type(dataset), len(dataset))\n",
    "# print(type(dataset[0]), len(dataset[0]))\n",
    "\n",
    "# print(type(dataset[0][0]), dataset[0][0].shape)\n",
    "# print(type(dataset[0][1]), dataset[0][1])\n",
    "# print(type(dataset[0][2]), dataset[0][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_dataloader_attack = torch.utils.data.DataLoader(\n",
    "#     dataset, batch_size=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_attack(eval_dataloader_attack, shadow_model, attack_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "\n",
    "You need to **submit your final prediction results on the test dataset `test.p` to [hai.huang@cispa.de](mailto:hai.huang@cispa.de) or [yugeng.liu@cispa.de](mailto:yugeng.liu@cispa.de)** to be checked by us. There should be **four** prediction result files in your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_PATH = 'test/pickle/cifar10/resnet34/test.p'\n",
    "\n",
    "# with open(DATA_PATH, \"rb\") as f:\n",
    "#     dataset = pickle.load(f)\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# prediction = [1 for i in range(len(dataset))]\n",
    "\n",
    "# np.save('./results/task0_resnet34_cifar10.npy', prediction)\n",
    "# np.save('./results/task1_mobilenetv2_cifar10.npy', prediction)\n",
    "# np.save('./results/task2_resnet34_tinyimagenet.npy', prediction)\n",
    "# np.save('./results/task3_mobilenetv2_tinyimagenet.npy', prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Name\n",
    "\n",
    "| File Name | Task ID | Dataset | Model |\n",
    "| :- | :-: | :-: | :-: |\n",
    "| `task0_resnet34_cifar10.npy` | 0 | CIFAR10 | ResNet34 |\n",
    "| `task1_mobilenetv2_cifar10.npy` | 1 | CIFAR10 | MobileNetV2 |\n",
    "| `task2_resnet34_tinyimagenet.npy` | 2 | Tiny ImageNet | ResNet34 |\n",
    "| `task3_mobilenetv2_tinyimagenet.npy` | 3 | Tiny ImageNet | MobileNetV2 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run.py --path ./results\n",
    "!python run.py --path ./c01hahu\n",
    "!python run.py --path ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
